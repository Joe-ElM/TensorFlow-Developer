{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GvJbBW_oDOwC"
   },
   "source": [
    "# Week 4: Handling Complex Images - Happy or Sad Dataset\n",
    "\n",
    "In this assignment you will be using the happy or sad dataset, which contains 80 images of emoji-like faces, 40 happy and 40 sad.\n",
    "\n",
    "Create a convolutional neural network that trains to 99.9% accuracy on these images,  which cancels training upon hitting this training accuracy threshold."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TIPS FOR SUCCESSFUL GRADING OF YOUR ASSIGNMENT:\n",
    "\n",
    "- All cells are frozen except for the ones where you need to submit your solutions or when explicitly mentioned you can interact with it.\n",
    "\n",
    "- You can add new cells to experiment but these will be omitted by the grader, so don't rely on newly created cells to host your solution code, use the provided places for this.\n",
    "\n",
    "- You can add the comment # grade-up-to-here in any graded cell to signal the grader that it must only evaluate up to that point. This is helpful if you want to check if you are on the right track even if you are not done with the whole assignment. Be sure to remember to delete the comment afterwards!\n",
    "\n",
    "- Avoid using global variables unless you absolutely have to. The grader tests your code in an isolated environment without running all cells from the top. As a result, global variables may be unavailable when scoring your submission. Global variables that are meant to be used will be defined in UPPERCASE.\n",
    "\n",
    "- To submit your notebook, save it and then click on the blue submit button at the beginning of the page."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "3NFuMFYXtwsT",
    "outputId": "723d6bc3-c7cd-491b-d6f8-49a2e404a0a2",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import base64\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "import unittests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore the data\n",
    "\n",
    "Begin by taking a look at some images of the dataset.\n",
    "\n",
    "All the images are contained within the `./data/` directory, notice that in this context the dot (`.`) means \"the current directory\". \n",
    "\n",
    "This `data/` directory contains two subdirectories `happy/` and `sad/` and each image is saved under the subdirectory related to the class it belongs to, take a look at the following tree for a more detailed view:\n",
    "\n",
    "```\n",
    ".\n",
    "└── data/\n",
    "    ├── happy/\n",
    "    │   ├── happy_image_1.png\n",
    "    │   ├── happy_image_2.png\n",
    "    │   └── ...\n",
    "    └── sad/\n",
    "        ├── sad_image_1.png\n",
    "        ├── sad_image_2.png\n",
    "        └── ...\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 369
    },
    "deletable": false,
    "editable": false,
    "id": "uaWTfp5Ox9E-",
    "outputId": "1a4b4b15-9a5f-4fd3-8c56-b32d47ae0893",
    "tags": [
     "graded"
    ]
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: './data/happy/'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m sad_dir \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(BASE_DIR, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msad/\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      5\u001b[0m fig, axs \u001b[38;5;241m=\u001b[39m plt\u001b[38;5;241m.\u001b[39msubplots(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m2\u001b[39m, figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m6\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m----> 6\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_img(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(happy_dir,\u001b[38;5;250m \u001b[39m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhappy_dir\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n\u001b[0;32m      7\u001b[0m axs[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mset_title(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mExample happy Face\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      9\u001b[0m axs[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mimshow(tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mload_img(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mos\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(sad_dir,\u001b[38;5;250m \u001b[39mos\u001b[38;5;241m.\u001b[39mlistdir(sad_dir)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m))\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: './data/happy/'"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 600x600 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "BASE_DIR = \"./data/\"\n",
    "happy_dir = os.path.join(BASE_DIR, \"happy/\")\n",
    "sad_dir = os.path.join(BASE_DIR, \"sad/\")\n",
    "\n",
    "fig, axs = plt.subplots(1, 2, figsize=(6, 6))\n",
    "axs[0].imshow(tf.keras.utils.load_img(f\"{os.path.join(happy_dir, os.listdir(happy_dir)[0])}\"))\n",
    "axs[0].set_title('Example happy Face')\n",
    "\n",
    "axs[1].imshow(tf.keras.utils.load_img(f\"{os.path.join(sad_dir, os.listdir(sad_dir)[0])}\"))\n",
    "axs[1].set_title('Example sad Face')\n",
    "\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is cool to be able to see examples of the images to better understand the problem-space you are dealing with. \n",
    "\n",
    "However there is still some relevant information that is missing such as the resolution of the image (although matplotlib renders the images in a grid providing a good idea of these values) and the maximum pixel value (this is important for normalizing these values). For this you can use some `tf.keras` utility functions as shown in the next cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Load the first example of a happy face\n",
    "sample_image  = tf.keras.utils.load_img(f\"{os.path.join(happy_dir, os.listdir(happy_dir)[0])}\")\n",
    "\n",
    "# Convert the image into its numpy array representation\n",
    "sample_array = tf.keras.utils.img_to_array(sample_image)\n",
    "\n",
    "print(f\"Each image has shape: {sample_array.shape}\")\n",
    "\n",
    "print(f\"The maximum pixel value used is: {np.max(sample_array)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the images have a resolution of 150x150. **This is very important because this will be the input size of the first layer in your network.** \n",
    "\n",
    "**The last dimension refers to each one of the 3 RGB (Red, Green, Blue) channels that are used to represent colored images.** So far, in the previous assignments you used black and white images so it is time to introduce some color!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the callback\n",
    "\n",
    "Since you already have coded the callback responsible for stopping training (once a desired level of accuracy is reached) in the previous two assignments this time it is already provided so you can focus on the other steps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "id": "X0UOFLauzIW4",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "class EarlyStoppingCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        if logs['accuracy'] >= 0.999:\n",
    "            self.model.stop_training = True\n",
    "            print(\"\\nReached 99.9% accuracy so cancelling training!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So far you have implemented an `EarlyStoppingCallback` by customizing the `on_epoch_end` method but there is a version of this callback already available within `tf.keras`. You might want to check out the [EarlyStopping](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks/EarlyStopping) callback, which has some extra functionality such as allowing you to save the best weights for your model and while at it take a look at all the other cool callbacks in the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/callbacks)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1: training_dataset\n",
    "\n",
    "Up until now, in the previous 3 assignments you have used numpy arrays to hold your training data, which is a valid input for Tensorflow models. However it is often a better practice to use `tf.data.Dataset` since this provides extra functionality. You can even create these out of numpy arrays and many other data sources. Be sure to check the [docs](https://www.tensorflow.org/api_docs/python/tf/data/Dataset) to learn more about this, as you will use this extensively in the next courses of the specialization.\n",
    "\n",
    "You have covered some ground already and it is now time for your first task! \n",
    "\n",
    "You will now use the images of happy and sad faces to create your training dataset. Previously you used some `tf.keras` utility functions to work with image data. Now you will use one of the most powerful ones which is `image_dataset_from_directory`. Be sure to check out the [docs](https://www.tensorflow.org/api_docs/python/tf/keras/utils/image_dataset_from_directory) to see how this function is used and how its behaviour can be tweaked by providing different arguments for it. Remember to scale the images using a [Rescaling](https://www.tensorflow.org/api_docs/python/tf/keras/layers/Rescaling) layer and to apply this to the dataset by using the [map](https://www.tensorflow.org/api_docs/python/tf/data/Dataset#map) method as you saw in the ungraded labs! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "cellView": "code",
    "deletable": false,
    "id": "rrGO8ObGzqht",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: training_dataset\n",
    "\n",
    "def training_dataset():\n",
    "    \"\"\"Creates the training dataset out of the training images. Pixel values should be normalized.\n",
    "\n",
    "    Returns:\n",
    "        tf.data.Dataset: The dataset including the images of happy and sad faces.\n",
    "    \"\"\"\n",
    "    \n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Specify the function to load images from a directory and pass in the appropriate arguments:\n",
    "    # - directory: should be a relative path to the directory containing the data. \n",
    "    #              You may hardcode this or use the previously defined global variable.\n",
    "    # - image_size: set this equal to the resolution of each image (excluding the color dimension)\n",
    "    # - batch_size: number of images the generator yields when asked for a next batch. Set this to 10.\n",
    "    # - class_mode: How the labels are represented. Should be one of \"binary\", \"categorical\" or \"int\".\n",
    "    #               Pick the one that better suits here given that the labels can only be two different values.\n",
    "    train_dataset = tf.keras.utils.image_dataset_from_directory(\n",
    "        directory=None,\n",
    "        image_size=None,\n",
    "        batch_size=None,\n",
    "        label_mode=None\n",
    "    )\n",
    "\n",
    "    # Define the rescaling layer (passing in the appropriate parameters)\n",
    "    rescale_layer = None\n",
    "\n",
    "    # Apply the rescaling by using the map method and a lambda\n",
    "    train_dataset_scaled = None\n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return train_dataset_scaled\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "deletable": false,
    "editable": false,
    "id": "L9uxJFQb1nOx",
    "outputId": "0c6ce535-7764-4bc0-a4a4-e6289a360b04",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# Save your generator in a variable\n",
    "train_data = training_dataset()\n",
    "\n",
    "for images, labels in train_data.take(1):\n",
    "    print(f\"Range for pixel values: {np.min(images[0]), np.max(images[0])}\")\n",
    "\n",
    "print(f\"train_data is an instance of tf.data.Dataset: {isinstance(train_data, tf.data.Dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "Found 80 files belonging to 2 classes.\n",
    "Range for pixel values: (0.0, 1.0)\n",
    "train_data is an instance of tf.data.Dataset: True\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_train_data(train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: create_and_compile_model\n",
    "\n",
    "Now that you have the training data ready it is time to define the model you will use to classify the happy and sad faces.\n",
    "\n",
    "**Your model should achieve an accuracy of 99.9% or more before 15 epochs to pass this assignment.**\n",
    "\n",
    "**Hints:**\n",
    "- The [Input](https://www.tensorflow.org/api_docs/python/tf/keras/Input) of your model should account for the shape of the data, which in this case is the size of each image plus the color dimension.\n",
    "\n",
    "\n",
    "\n",
    "- The last layer of your network should take into account the number of classes you are trying to predict and be compatible with the `label_mode` you defined in the previous exercise.\n",
    "\n",
    "- The selection of the loss function should take into consideration the `label_mode` you defined in the previous exercise and the last layer of your network. For a list of available loss functions click [here](https://www.tensorflow.org/api_docs/python/tf/keras/losses).\n",
    "\n",
    "- Remember to set the `accuracy` metric as the callback expects it.\n",
    "\n",
    "- You can try any architecture for the network but keep in mind that the model will work best with 3 convolutional layers. \n",
    "\n",
    "\n",
    "- In case you need extra help you can check out some tips at the end of this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "id": "eUcNTpra1FK0",
    "tags": [
     "graded"
    ]
   },
   "outputs": [],
   "source": [
    "# GRADED FUNCTION: create_and_compile_model\n",
    "\n",
    "def create_and_compile_model():\n",
    "    \"\"\"Creates, compiles and trains the model to predict happy from sad faces.\n",
    "\n",
    "    Returns:\n",
    "        tf.keras.Model: The model that will be trained to predict predict happy and sad faces.\n",
    "    \"\"\"\n",
    "\n",
    "    ### START CODE HERE ###\n",
    "\n",
    "    # Define the model\n",
    "    model = tf.keras.models.Sequential([ \n",
    "\t\tNone\n",
    "    ]) \n",
    "\n",
    "    # Compile the model\n",
    "    # Remember to set an appropriate loss function, optimizer and metrics \n",
    "    None.None(\n",
    "        loss=None,\n",
    "        optimizer=None,\n",
    "        metrics=None\n",
    "    ) \n",
    "    \n",
    "    ### END CODE HERE ###\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell allows you to check the number of total and trainable parameters of your model and prompts a warning in case these exceeds those of a reference solution, this serves the following 3 purposes listed in order of priority:\n",
    "\n",
    "- Helps you prevent crashing the kernel during training.\n",
    "\n",
    "- Helps you avoid longer-than-necessary training times.\n",
    "\n",
    "- Provides a reasonable estimate of the size of your model. In general you will usually prefer smaller models given that they accomplish their goal successfully.\n",
    "\n",
    "**Notice that this is just informative** and may be very well below the actual limit for size of the model necessary to crash the kernel. So even if you exceed this reference you are probably fine. However, **if the kernel crashes during training or it is taking a very long time and your model is larger than the reference, come back here and try to get the number of parameters closer to the reference.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Save untrained model in a variable\n",
    "model = create_and_compile_model()\n",
    "\n",
    "# Check parameter count against a reference solution\n",
    "unittests.parameter_count(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check that the architecture you used is compatible with the dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Get the first batch of images and labels\n",
    "for images, labels in train_data.take(1):\n",
    "\texample_batch_images = images\n",
    "\texample_batch_labels = labels\n",
    "\n",
    "try:\n",
    "\tmodel.evaluate(example_batch_images, example_batch_labels, verbose=False)\n",
    "except:\n",
    "\tprint(\"Your model is not compatible with the dataset you defined earlier. Check that the loss function, last layer and label_mode are compatible with one another.\")\n",
    "else:\n",
    "\tpredictions = model.predict(example_batch_images, verbose=False)\n",
    "\tprint(f\"predictions have shape: {predictions.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "```\n",
    "predictions have shape: (batch_size, n_units)\n",
    "```\n",
    "\n",
    "Where `batch_size` is the one you defined in the previous exercise (should be 10) and `n_units` is the number of units of the last layer of your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_create_and_compile_model(create_and_compile_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice that when using the `fit` method to train the model, you can pass in the whole `train_data` without explicitly separating features from labels. This is because `train_data` is a `tf.data.Dataset` and this operation is supported for objects of this class. For more info click [here](https://www.tensorflow.org/api_docs/python/tf/keras/Model#fit)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "outputs": [],
   "source": [
    "# Get the training history from your model\n",
    "training_history = model.fit(\n",
    "\tx=train_data,\n",
    "    epochs=15,\n",
    "    callbacks=[EarlyStoppingCallback()]\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "\n",
    "`Reached 99.9% accuracy so cancelling training!` printed out before reaching 15 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Test your code!\n",
    "unittests.test_training_history(training_history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Need more help?\n",
    "\n",
    "Run the following cell to see some extra tips for the model's architecture and compilation parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false,
    "editable": false,
    "tags": []
   },
   "outputs": [],
   "source": [
    "encoded_answer = \"ClNvbWUgaGVscGZ1bCB0aXBzIGluIGNhc2UgeW91IGFyZSBzdHVjazoKCiAgICAtIFRoZSBpbnB1dCBzaG91bGQgYmUgYSB0Zi5rZXJhcy5JbnB1dCB3aXRoIGEgc2hhcGUgdGhhdCBtYXRjaGVzIAogICAgdGhhdCBvZiBldmVyeSBpbWFnZSBpbiB0aGUgdHJhaW5pbmcgc2V0IChpbmNsdWRpbmcgdGhlIGNvbG9yIGRpbWVuc2lvbikKICAgIAogICAgLSBBIGdvb2QgbGF5ZXIgKGFmdGVyIHRoZSBJbnB1dCkgd291bGQgYmUgYSBDb252MkQgbGF5ZXIKICAgIAogICAgLSBUaGUgbW9kZWwgd2lsbCB3b3JrIGJlc3Qgd2l0aCAzIGNvbnZvbHV0aW9uYWwgbGF5ZXJzCiAgICAKICAgIC0gVGhlcmUgc2hvdWxkIGJlIGEgRmxhdHRlbiBsYXllciBpbiBiZXR3ZWVuIGNvbnZvbHV0aW9uYWwgYW5kIGRlbnNlIGxheWVycwogICAgCiAgICAtIFRoZSBmaW5hbCBsYXllciBzaG91bGQgYmUgYSBEZW5zZSBsYXllciB3aXRoIHRoZSBudW1iZXIgb2YgdW5pdHMgYW5kIAogICAgYWN0aXZhdGlvbiBmdW5jdGlvbiB0aGF0IHN1cHBvcnRzIGJpbmFyeSBjbGFzc2lmaWNhdGlvbi4KCiAgICAtIEFkYW0gaXMgYSBnb29kIG9wdGltaXplciBpbiB0aGlzIGNhc2UuCgogICAgLSBBYm91dCBsb3NzIGZ1bmN0aW9uczoKCiAgICAgICAgLSBTcGFyc2VDYXRlZ29yaWNhbENyb3NzZW50cm9weSB3aWxsIHJlcXVpcmUgbGFiZWxfbW9kZSB0byBiZSAnaW50JyBvciAnYmluYXJ5JyAKICAgICAgICBhbmQgdGhlIGxhc3QgbGF5ZXIgc2hvdWxkIGhhdmUgdHdvIHVuaXRzIHdpdGggYSAnc29mdG1heCcgYWN0aXZhdGlvbiBmdW5jdGlvbi4KCiAgICAgICAgLSBCaW5hcnlDcm9zc2VudHJvcHkgd2lsbCByZXF1aXJlIGxhYmVsX21vZGUgdG8gYmUgJ2ludCcgb3IgJ2JpbmFyeScgCiAgICAgICAgYW5kIHRoZSBsYXN0IGxheWVyIHNob3VsZCBoYXZlIG9ubHkgb25lIHVuaXQgd2l0aCBhbiBhY3RpdmF0aW9uIGZ1bmN0aW9uIHN1Y2ggYXMgJ3NpZ21vaWQnLgoKICAgICAgICAtIENhdGVnb3JpY2FsQ3Jvc3NlbnRyb3B5IHdpbGwgcmVxdWlyZSBsYWJlbF9tb2RlIHRvIGJlICdjYXRlZ29yaWNhbCcKICAgICAgICBhbmQgdGhlIGxhc3QgbGF5ZXIgc2hvdWxkIGhhdmUgdHdvIHVuaXRzIHdpdGggYSAnc29mdG1heCcgYWN0aXZhdGlvbiBmdW5jdGlvbi4K==\"\n",
    "encoded_answer = encoded_answer.encode('ascii')\n",
    "answer = base64.b64decode(encoded_answer)\n",
    "answer = answer.decode('ascii')\n",
    "\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Congratulations on finishing the last assignment of this course!**\n",
    "\n",
    "You have successfully implemented a CNN to assist you in the classification task for complex images. Nice job!\n",
    "\n",
    "**Keep it up!**"
   ]
  }
 ],
 "metadata": {
  "grader_version": "1",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
